{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIME Tutorial\n",
    "\n",
    "### VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain\n",
    "\n",
    "- Paper: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "  \"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "  Neural Information Processing Systems (NeurIPS), 2020.\n",
    "\n",
    "- Paper link: TBD\n",
    "\n",
    "- Last updated Date: October 11th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of self- and semi-supervised learning for tabular domain using MNIST database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall numpy\n",
    "# pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\anaconda33\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "c = tf.add(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(c)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/VIME.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary packages and functions call\n",
    "\n",
    "- data_loader: MNIST dataset loading and preprocessing\n",
    "- supervised_models: supervised learning models (Logistic regression, XGBoost, and Multi-layer Perceptron)\n",
    "\n",
    "- vime_self: Self-supervised learning part of VIME framework\n",
    "- vime_semi: Semi-supervised learning part of VIME framework\n",
    "- vime_utils: Some utility functions for VIME framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\georg\\anaconda33\\lib\\site-packages (1.7.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\georg\\appdata\\roaming\\python\\python39\\site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\georg\\anaconda33\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: tf-slim in c:\\users\\georg\\anaconda33\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\georg\\anaconda33\\lib\\site-packages (from tf-slim) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n",
    "! pip install tf-slim\n",
    "# ! pip install fancyimpute statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "from data_loader import load_mnist_data\n",
    "from data_loader import load_excel_data\n",
    "from data_loader import load_excel_data_multi_class\n",
    "\n",
    "from supervised_models import logit, xgb_model, mlp\n",
    "\n",
    "from vime_self import vime_self\n",
    "from vime_self import get_encoder\n",
    "from vime_self import vime_self_fnn\n",
    "from vime_semi import vime_semi\n",
    "from vime_utils import perf_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# x_train, y_train, x_unlab, x_test, y_test = load_excel_data('TCGA_InfoWithGrade.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_unlab.shape)\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and define output\n",
    "\n",
    "-   label_no: Number of labeled data to be used\n",
    "-   model_sets: supervised model set (mlp, logit, or xgboost)\n",
    "-   p_m: corruption probability for self-supervised learning\n",
    "-   alpha: hyper-parameter to control the weights of feature and mask losses\n",
    "-   K: number of augmented samples\n",
    "-   beta: hyperparameter to control supervised and unsupervised loss\n",
    "-   label_data_rate: ratio of labeled data\n",
    "-   metric: prediction performance metric (either acc or auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental parameters\n",
    "label_no = 1000  \n",
    "model_sets = ['logit','xgboost','mlp']\n",
    "  \n",
    "# Hyper-parameters\n",
    "p_m = 0.3\n",
    "alpha = 2.0\n",
    "K = 3\n",
    "beta = 1.0\n",
    "label_data_rate = 0.1\n",
    "\n",
    "# Metric\n",
    "metric = 'acc'\n",
    "  \n",
    "# Define output\n",
    "results = np.zeros([len(model_sets)+2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Load original MNIST dataset and preprocess the loaded data.\n",
    "- Only select the subset of data as the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# x_train, y_train, x_unlab, x_test, y_test = load_mnist_data(label_data_rate)\n",
    "    \n",
    "# # Use subset of labeled data\n",
    "# x_train = x_train[:label_no, :]\n",
    "# y_train = y_train[:label_no, :]\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_unlab.shape)\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (56, 450)\n",
      "y_train shape: (56, 2)\n",
      "x_unlab shape: (84, 450)\n",
      "x_test shape: (34, 450)\n",
      "y_test shape: (34, 2)\n"
     ]
    }
   ],
   "source": [
    "# # Assuming the function load_excel_data_multi_class is defined in the same Jupyter notebook or imported properly\n",
    "\n",
    "# # Call the function\n",
    "# x_train, y_train , x_unlab, x_test, y_test = load_excel_data_multi_class('DARWIN.xlsx', label_data_rate=0.4, test_data_rate=0.2)\n",
    "\n",
    "# # Print the shapes of the returned data\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"x_unlab shape:\", x_unlab.shape)\n",
    "# print(\"x_test shape:\", x_test.shape)\n",
    "# print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# x_unlab = x_unlab.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "x_train, y_train, x_unlab, x_test, y_test = load_excel_data('TCGA_InfoWithGrade.xlsx')\n",
    "\n",
    "# Ensure everything is a numpy ndarray\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "x_unlab = x_unlab.to_numpy()\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = y_test.to_numpy().reshape(-1, 1) \n",
    "\n",
    "# Convert y_train and y_test into one-hot vectors\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Apply your transformations\n",
    "label_no = 1000  # Or any appropriate value\n",
    "x_train = x_train[:label_no, :]\n",
    "y_train = y_train[:label_no, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 213us/sample - loss: 252.4229 - mask_loss: 2.2257 - feature_loss: 123.5196\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 251.6269 - mask_loss: 1.6146 - feature_loss: 126.2494\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 251.1386 - mask_loss: 1.2287 - feature_loss: 124.1635\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 250.8461 - mask_loss: 0.9982 - feature_loss: 124.8382\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 250.6121 - mask_loss: 0.8281 - feature_loss: 124.7177\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 250.3760 - mask_loss: 0.6934 - feature_loss: 119.6815\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 249.1443 - mask_loss: 0.5958 - feature_loss: 126.6268\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 243.4113 - mask_loss: 0.5245 - feature_loss: 121.3384\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 241.8837 - mask_loss: 0.4537 - feature_loss: 121.9581\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 241.5255 - mask_loss: 0.4156 - feature_loss: 119.2855\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8562874251497006\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.7589\n",
      "Iteration: 100/1000, Current loss: 0.6124\n",
      "Iteration: 200/1000, Current loss: 0.604\n",
      "Iteration: 300/1000, Current loss: 0.5864\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: logit, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "VIME-Self Performance: 0.8562874251497006\n",
      "VIME Performance: 0.874251497005988\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.844311377245509\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 186us/sample - loss: 236.4355 - mask_loss: 2.5092 - feature_loss: 118.9684\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 235.6587 - mask_loss: 1.9137 - feature_loss: 116.7450\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 235.1852 - mask_loss: 1.5236 - feature_loss: 114.7875\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 234.8303 - mask_loss: 1.2487 - feature_loss: 116.5854\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 234.5498 - mask_loss: 1.0252 - feature_loss: 116.5533\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 234.3369 - mask_loss: 0.8459 - feature_loss: 114.9477\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 234.1649 - mask_loss: 0.7161 - feature_loss: 117.1869\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 234.0387 - mask_loss: 0.6142 - feature_loss: 117.0296\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 233.9489 - mask_loss: 0.5377 - feature_loss: 117.4459\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 233.8862 - mask_loss: 0.4914 - feature_loss: 116.7221\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.7544910179640718\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.6921\n",
      "Iteration: 100/1000, Current loss: 0.4742\n",
      "Iteration: 200/1000, Current loss: 0.432\n",
      "Iteration: 300/1000, Current loss: 0.418\n",
      "Iteration: 400/1000, Current loss: 0.4083\n",
      "Iteration: 500/1000, Current loss: 0.4354\n",
      "Iteration: 600/1000, Current loss: 0.4011\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.844311377245509\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.844311377245509\n",
      "VIME-Self Performance: 0.7544910179640718\n",
      "VIME Performance: 0.844311377245509\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8203592814371258\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.844311377245509\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 176us/sample - loss: 248.4052 - mask_loss: 2.6727 - feature_loss: 122.7296\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 247.8938 - mask_loss: 2.2741 - feature_loss: 122.9746\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 247.5396 - mask_loss: 1.9998 - feature_loss: 123.7662\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 247.1334 - mask_loss: 1.8028 - feature_loss: 125.7728\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 14us/sample - loss: 246.3353 - mask_loss: 1.6243 - feature_loss: 119.4656\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 244.2271 - mask_loss: 1.5171 - feature_loss: 120.3969\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 241.3868 - mask_loss: 1.4279 - feature_loss: 121.9821\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 239.7316 - mask_loss: 1.3047 - feature_loss: 118.9977\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 238.9246 - mask_loss: 1.2196 - feature_loss: 117.7068\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 238.4370 - mask_loss: 1.1284 - feature_loss: 117.5419\n",
      "Encoder output shape: (?, 23)\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 51: early stopping\n",
      "VIME-Self Performance: 0.3712574850299401\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.004\n",
      "Iteration: 100/1000, Current loss: 0.5436\n",
      "Iteration: 200/1000, Current loss: 0.4901\n",
      "Iteration: 300/1000, Current loss: 0.4936\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8622754491017964\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8203592814371258\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.844311377245509\n",
      "VIME-Self Performance: 0.3712574850299401\n",
      "VIME Performance: 0.8622754491017964\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8922155688622755\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.844311377245509\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.9041916167664671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 174us/sample - loss: 244.3403 - mask_loss: 2.4002 - feature_loss: 122.4014\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 243.8844 - mask_loss: 2.0218 - feature_loss: 123.0928\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 243.5953 - mask_loss: 1.8121 - feature_loss: 120.6292\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 243.3552 - mask_loss: 1.5955 - feature_loss: 119.0018\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 243.1366 - mask_loss: 1.4433 - feature_loss: 119.9325\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 242.9295 - mask_loss: 1.2985 - feature_loss: 124.2878\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 242.7368 - mask_loss: 1.2084 - feature_loss: 123.9992\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 242.5518 - mask_loss: 1.0634 - feature_loss: 119.0378\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 242.3641 - mask_loss: 0.9590 - feature_loss: 120.9738\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 11us/sample - loss: 242.1550 - mask_loss: 0.8745 - feature_loss: 121.2493\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8682634730538922\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.2692\n",
      "Iteration: 100/1000, Current loss: 0.5195\n",
      "Iteration: 200/1000, Current loss: 0.4513\n",
      "Iteration: 300/1000, Current loss: 0.4536\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8922155688622755\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8922155688622755\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.844311377245509\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.9041916167664671\n",
      "VIME-Self Performance: 0.8682634730538922\n",
      "VIME Performance: 0.8922155688622755\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8562874251497006\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 182us/sample - loss: 236.7931 - mask_loss: 2.4573 - feature_loss: 116.0656\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 233.6846 - mask_loss: 1.9576 - feature_loss: 115.6929\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 232.9647 - mask_loss: 1.6672 - feature_loss: 114.9028\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 232.5202 - mask_loss: 1.4635 - feature_loss: 116.9089\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 232.1653 - mask_loss: 1.2807 - feature_loss: 117.6921\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 231.8726 - mask_loss: 1.1301 - feature_loss: 111.4314\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 231.6486 - mask_loss: 1.0362 - feature_loss: 114.5318\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 231.4733 - mask_loss: 0.9174 - feature_loss: 115.7625\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 231.3322 - mask_loss: 0.8127 - feature_loss: 112.5720\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 231.2116 - mask_loss: 0.7183 - feature_loss: 112.6628\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8682634730538922\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.5636\n",
      "Iteration: 100/1000, Current loss: 0.5206\n",
      "Iteration: 200/1000, Current loss: 0.4056\n",
      "Iteration: 300/1000, Current loss: 0.3353\n",
      "Iteration: 400/1000, Current loss: 0.3145\n",
      "Iteration: 500/1000, Current loss: 0.3069\n",
      "Iteration: 600/1000, Current loss: 0.2945\n",
      "Iteration: 700/1000, Current loss: 0.2908\n",
      "Iteration: 800/1000, Current loss: 0.2786\n",
      "Iteration: 900/1000, Current loss: 0.3036\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: logit, Performance: 0.874251497005988\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8562874251497006\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "VIME-Self Performance: 0.8682634730538922\n",
      "VIME Performance: 0.874251497005988\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8143712574850299\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8023952095808383\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 203us/sample - loss: 250.0155 - mask_loss: 4.2587 - feature_loss: 121.7390\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 247.5816 - mask_loss: 3.6148 - feature_loss: 120.6000\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 246.7909 - mask_loss: 3.1978 - feature_loss: 120.8329\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 18us/sample - loss: 246.2246 - mask_loss: 2.8447 - feature_loss: 119.5522\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 245.7211 - mask_loss: 2.4926 - feature_loss: 123.3774\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 245.2507 - mask_loss: 2.1981 - feature_loss: 123.4003\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 244.8528 - mask_loss: 1.8874 - feature_loss: 124.4701\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 244.5344 - mask_loss: 1.6070 - feature_loss: 119.6944\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 244.2577 - mask_loss: 1.3980 - feature_loss: 121.7482\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 244.0086 - mask_loss: 1.2009 - feature_loss: 121.1420\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.7844311377245509\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.0623\n",
      "Iteration: 100/1000, Current loss: 0.5476\n",
      "Iteration: 200/1000, Current loss: 0.388\n",
      "Iteration: 300/1000, Current loss: 0.3858\n",
      "Iteration: 400/1000, Current loss: 0.3306\n",
      "Iteration: 500/1000, Current loss: 0.3272\n",
      "Iteration: 600/1000, Current loss: 0.3251\n",
      "Iteration: 700/1000, Current loss: 0.3119\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8203592814371258\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8143712574850299\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8023952095808383\n",
      "VIME-Self Performance: 0.7844311377245509\n",
      "VIME Performance: 0.8203592814371258\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8802395209580839\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8622754491017964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 357us/sample - loss: 242.9201 - mask_loss: 3.1036 - feature_loss: 118.3263\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 26us/sample - loss: 236.7348 - mask_loss: 2.5243 - feature_loss: 118.5647\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 28us/sample - loss: 234.5211 - mask_loss: 2.1550 - feature_loss: 118.2777\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 24us/sample - loss: 233.8288 - mask_loss: 1.8621 - feature_loss: 112.4593\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 30us/sample - loss: 233.3810 - mask_loss: 1.6387 - feature_loss: 113.3917\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 21us/sample - loss: 233.0216 - mask_loss: 1.4340 - feature_loss: 119.0712\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 22us/sample - loss: 232.7387 - mask_loss: 1.2595 - feature_loss: 115.4335\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 232.5360 - mask_loss: 1.1085 - feature_loss: 114.2190\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 232.3660 - mask_loss: 0.9996 - feature_loss: 117.0322\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 18us/sample - loss: 232.2105 - mask_loss: 0.9098 - feature_loss: 116.6864\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.6107784431137725\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.0418\n",
      "Iteration: 100/1000, Current loss: 0.6372\n",
      "Iteration: 200/1000, Current loss: 0.6218\n",
      "Iteration: 300/1000, Current loss: 0.6186\n",
      "Iteration: 400/1000, Current loss: 0.6147\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.7844311377245509\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8802395209580839\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8323353293413174\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8622754491017964\n",
      "VIME-Self Performance: 0.6107784431137725\n",
      "VIME Performance: 0.7844311377245509\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8862275449101796\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.844311377245509\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 197us/sample - loss: 224.8752 - mask_loss: 2.2336 - feature_loss: 108.7098\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 222.8580 - mask_loss: 1.9432 - feature_loss: 107.4152\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 219.1044 - mask_loss: 1.7892 - feature_loss: 107.6304\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 217.3298 - mask_loss: 1.6393 - feature_loss: 109.5266\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 216.6278 - mask_loss: 1.5291 - feature_loss: 107.4566\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 216.2208 - mask_loss: 1.4107 - feature_loss: 105.2226\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 20us/sample - loss: 215.9303 - mask_loss: 1.3268 - feature_loss: 109.5096\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 215.7012 - mask_loss: 1.1991 - feature_loss: 106.8325\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 215.5082 - mask_loss: 1.1300 - feature_loss: 105.7145\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 215.3374 - mask_loss: 1.0541 - feature_loss: 109.8031\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8622754491017964\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.784\n",
      "Iteration: 100/1000, Current loss: 0.7919\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.6646706586826348\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8862275449101796\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.844311377245509\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8562874251497006\n",
      "VIME-Self Performance: 0.8622754491017964\n",
      "VIME Performance: 0.6646706586826348\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8922155688622755\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8562874251497006\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8862275449101796\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 198us/sample - loss: 183.9968 - mask_loss: 1.9001 - feature_loss: 90.3854\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 183.1388 - mask_loss: 1.5505 - feature_loss: 88.8788\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 182.7002 - mask_loss: 1.3242 - feature_loss: 90.3860\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 16us/sample - loss: 182.3691 - mask_loss: 1.1421 - feature_loss: 94.3650\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 14us/sample - loss: 182.1060 - mask_loss: 0.9685 - feature_loss: 89.4988\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 181.8838 - mask_loss: 0.8325 - feature_loss: 87.7969\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 181.6902 - mask_loss: 0.7328 - feature_loss: 90.6929\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 181.5179 - mask_loss: 0.6255 - feature_loss: 90.3941\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 181.3681 - mask_loss: 0.5493 - feature_loss: 88.5928\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 181.2408 - mask_loss: 0.4792 - feature_loss: 90.2233\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8023952095808383\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.7154\n",
      "Iteration: 100/1000, Current loss: 0.393\n",
      "Iteration: 200/1000, Current loss: 0.3392\n",
      "Iteration: 300/1000, Current loss: 0.3206\n",
      "Iteration: 400/1000, Current loss: 0.304\n",
      "Iteration: 500/1000, Current loss: 0.2877\n",
      "Iteration: 600/1000, Current loss: 0.2945\n",
      "Iteration: 700/1000, Current loss: 0.2676\n",
      "Iteration: 800/1000, Current loss: 0.2691\n",
      "Iteration: 900/1000, Current loss: 0.2603\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8682634730538922\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8922155688622755\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.8562874251497006\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8862275449101796\n",
      "VIME-Self Performance: 0.8023952095808383\n",
      "VIME Performance: 0.8682634730538922\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8502994011976048\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7485029940119761\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8502994011976048\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 221us/sample - loss: 242.5736 - mask_loss: 1.5493 - feature_loss: 120.6715\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/538 [==============================] - 0s 20us/sample - loss: 241.7174 - mask_loss: 1.3069 - feature_loss: 120.0816\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 20us/sample - loss: 241.3524 - mask_loss: 1.1442 - feature_loss: 121.3207\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 20us/sample - loss: 241.0915 - mask_loss: 1.0408 - feature_loss: 121.4893\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 240.8774 - mask_loss: 0.9385 - feature_loss: 120.5436\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 240.6897 - mask_loss: 0.8395 - feature_loss: 120.3548\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 240.5180 - mask_loss: 0.7638 - feature_loss: 116.9954\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 240.3599 - mask_loss: 0.6929 - feature_loss: 120.9119\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 240.2270 - mask_loss: 0.6348 - feature_loss: 119.9243\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 240.1213 - mask_loss: 0.5866 - feature_loss: 120.5325\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.7664670658682635\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.6694\n",
      "Iteration: 100/1000, Current loss: 0.4913\n",
      "Iteration: 200/1000, Current loss: 0.4457\n",
      "Iteration: 300/1000, Current loss: 0.4082\n",
      "Iteration: 400/1000, Current loss: 0.4285\n",
      "Iteration: 500/1000, Current loss: 0.4359\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8083832335329342\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8502994011976048\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7485029940119761\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.8502994011976048\n",
      "VIME-Self Performance: 0.7664670658682635\n",
      "VIME Performance: 0.8083832335329342\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7964071856287425\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.874251497005988\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 221us/sample - loss: 239.7262 - mask_loss: 2.2491 - feature_loss: 118.0339\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 239.1754 - mask_loss: 1.8589 - feature_loss: 118.2046\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 19us/sample - loss: 238.8338 - mask_loss: 1.5792 - feature_loss: 116.9198\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 238.5687 - mask_loss: 1.4031 - feature_loss: 117.3559\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 238.3300 - mask_loss: 1.2368 - feature_loss: 118.7454\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 19us/sample - loss: 238.1156 - mask_loss: 1.0819 - feature_loss: 119.6164\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 237.9268 - mask_loss: 0.9631 - feature_loss: 116.5595\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 237.7634 - mask_loss: 0.8352 - feature_loss: 119.1125\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 237.6160 - mask_loss: 0.7171 - feature_loss: 118.7481\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 237.4939 - mask_loss: 0.6311 - feature_loss: 115.3340\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8023952095808383\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 1.3842\n",
      "Iteration: 100/1000, Current loss: 0.4125\n",
      "Iteration: 200/1000, Current loss: 0.3439\n",
      "Iteration: 300/1000, Current loss: 0.3402\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.7544910179640718\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8383233532934131\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7964071856287425\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.874251497005988\n",
      "VIME-Self Performance: 0.8023952095808383\n",
      "VIME Performance: 0.7544910179640718\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "Total train data length: 672\n",
      "Label data length (from train_idx): 134\n",
      "Unlabel data length (from train_idx): 538\n",
      "Provided label_data_rate: 0.2\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8682634730538922\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7724550898203593\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.7964071856287425\n",
      "Train on 538 samples\n",
      "Epoch 1/10\n",
      "538/538 [==============================] - 0s 196us/sample - loss: 223.9366 - mask_loss: 2.0546 - feature_loss: 114.9934\n",
      "Epoch 2/10\n",
      "538/538 [==============================] - 0s 19us/sample - loss: 223.0836 - mask_loss: 1.7821 - feature_loss: 108.3710\n",
      "Epoch 3/10\n",
      "538/538 [==============================] - 0s 19us/sample - loss: 222.6997 - mask_loss: 1.6530 - feature_loss: 109.8097\n",
      "Epoch 4/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 222.4145 - mask_loss: 1.4971 - feature_loss: 107.3914\n",
      "Epoch 5/10\n",
      "538/538 [==============================] - 0s 18us/sample - loss: 222.2041 - mask_loss: 1.3716 - feature_loss: 106.4302\n",
      "Epoch 6/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 222.0352 - mask_loss: 1.2537 - feature_loss: 108.9058\n",
      "Epoch 7/10\n",
      "538/538 [==============================] - 0s 19us/sample - loss: 221.8818 - mask_loss: 1.1290 - feature_loss: 108.6753\n",
      "Epoch 8/10\n",
      "538/538 [==============================] - 0s 13us/sample - loss: 221.7336 - mask_loss: 0.9900 - feature_loss: 105.7366\n",
      "Epoch 9/10\n",
      "538/538 [==============================] - 0s 17us/sample - loss: 221.5799 - mask_loss: 0.8924 - feature_loss: 111.1990\n",
      "Epoch 10/10\n",
      "538/538 [==============================] - 0s 15us/sample - loss: 221.4075 - mask_loss: 0.7788 - feature_loss: 111.2757\n",
      "Encoder output shape: (?, 23)\n",
      "VIME-Self Performance: 0.8143712574850299\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.7604\n",
      "Iteration: 100/1000, Current loss: 0.4803\n",
      "Iteration: 200/1000, Current loss: 0.3193\n",
      "Iteration: 300/1000, Current loss: 0.2992\n",
      "Iteration: 400/1000, Current loss: 0.2771\n",
      "Iteration: 500/1000, Current loss: 0.2726\n",
      "Iteration: 600/1000, Current loss: 0.2648\n",
      "Iteration: 700/1000, Current loss: 0.2735\n",
      "Iteration: 800/1000, Current loss: 0.2664\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.8502994011976048\n",
      "Supervised Performance, Model Name: logit, Performance: 0.8682634730538922\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7724550898203593\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.7964071856287425\n",
      "VIME-Self Performance: 0.8143712574850299\n",
      "VIME Performance: 0.8502994011976048\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "# all code together:\n",
    "#Ignore unless you want to try out all the modes and missingness rates at once!\n",
    "listMODES = [ 'mean','median','mode','random_sampling']\n",
    "listRATES = [0.05, 0.15, 0.4]\n",
    "\n",
    "for mode in listMODES:\n",
    "    for rate in listRATES:  \n",
    "\n",
    "\n",
    "        from keras.utils import to_categorical\n",
    "\n",
    "        x_train, y_train, x_unlab, x_test, y_test = load_excel_data('TCGA_InfoWithGrade.xlsx')\n",
    "\n",
    "        # Ensure everything is a numpy ndarray\n",
    "        x_train = x_train.to_numpy()\n",
    "        y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "        x_unlab = x_unlab.to_numpy()\n",
    "        x_test = x_test.to_numpy()\n",
    "        y_test = y_test.to_numpy().reshape(-1, 1) \n",
    "\n",
    "        # Convert y_train and y_test into one-hot vectors\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_test = to_categorical(y_test)\n",
    "\n",
    "        # Apply your transformations\n",
    "        label_no = 1000  # Or any appropriate value\n",
    "        x_train = x_train[:label_no, :]\n",
    "        y_train = y_train[:label_no, :]\n",
    "        \n",
    "        old_data = x_unlab\n",
    "\n",
    "        import numpy as np\n",
    "\n",
    "        def introduce_missingness(data, percentage=0.05, mechanism='MCAR'):\n",
    "            \"\"\"\n",
    "            Introduce missingness into a numpy array based on a specific mechanism.\n",
    "\n",
    "            Parameters:\n",
    "            - data: numpy array\n",
    "            - percentage: fraction of data that should be missing\n",
    "            - mechanism: 'MCAR', 'MAR', or 'MNAR'\n",
    "\n",
    "            Returns:\n",
    "            - modified_data: numpy array with missing values\n",
    "            \"\"\"\n",
    "\n",
    "            # Ensure data isn't an empty array\n",
    "            if data.size == 0:\n",
    "                return data\n",
    "\n",
    "            # Convert percentage to a fraction of total data values\n",
    "            total_values = data.size\n",
    "            missing_values = int(total_values * percentage)\n",
    "            values_made_missing = 0\n",
    "\n",
    "            # MCAR\n",
    "            if mechanism == 'MCAR':\n",
    "                # Randomly pick indices to set as missing\n",
    "                missing_indices = np.random.choice(total_values, missing_values, replace=False)\n",
    "                np.put(data, missing_indices, np.nan)\n",
    "                return data\n",
    "\n",
    "            # MAR\n",
    "            if mechanism == 'MAR':\n",
    "                while values_made_missing < missing_values:\n",
    "                    # Pick two features randomly\n",
    "                    feature_1 = np.random.choice(data.shape[1], 1)\n",
    "                    feature_2 = np.random.choice(data.shape[1], 1)\n",
    "                    while feature_1 == feature_2:\n",
    "                        feature_2 = np.random.choice(data.shape[1], 1)\n",
    "\n",
    "                    threshold = np.mean(data[:, feature_1])\n",
    "                    # Make values in feature_2 missing based on values in feature_1\n",
    "                    potential_missing_indices = np.where(data[:, feature_1] > threshold)\n",
    "                    remaining_missing = missing_values - values_made_missing\n",
    "                    sample_size = min(len(potential_missing_indices[0]), remaining_missing)\n",
    "                    if sample_size == 0:\n",
    "                        continue\n",
    "                    sample_missing = np.random.choice(len(potential_missing_indices[0]), sample_size, replace=False)\n",
    "                    data[potential_missing_indices[0][sample_missing], feature_2] = np.nan\n",
    "                    values_made_missing += sample_size\n",
    "                return data\n",
    "\n",
    "            # MNAR\n",
    "            if mechanism == 'MNAR':\n",
    "                feature = np.random.choice(data.shape[1], 1)\n",
    "                threshold = np.mean(data[:, feature])\n",
    "                potential_missing_indices = np.where(data[:, feature] > threshold)\n",
    "                sample_size = min(len(potential_missing_indices[0]), missing_values)\n",
    "                sample_missing = np.random.choice(len(potential_missing_indices[0]), sample_size, replace=False)\n",
    "                data[potential_missing_indices[0][sample_missing], feature] = np.nan\n",
    "                return data\n",
    "\n",
    "        # Sample usage\n",
    "\n",
    "        # data = np.random.rand(100, 5)\n",
    "        x_unlab = introduce_missingness(x_unlab, percentage=rate, mechanism='MCAR')\n",
    "        \n",
    "        import numpy as np\n",
    "        from sklearn.impute import SimpleImputer, KNNImputer\n",
    "        from sklearn.experimental import enable_iterative_imputer\n",
    "        from sklearn.impute import IterativeImputer\n",
    "\n",
    "        def impute_data(data, method='mean'):\n",
    "            # Find the indices of the missing values\n",
    "            missing_values_indices = np.argwhere(np.isnan(data))\n",
    "\n",
    "            if method == 'mean':\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "            elif method == 'median':\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "            elif method == 'mode':\n",
    "                imputer = SimpleImputer(strategy='most_frequent')\n",
    "            elif method == 'knn':\n",
    "                imputer = KNNImputer(n_neighbors=5)\n",
    "            elif method == 'iterative' or method == 'regression':\n",
    "                imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "            elif method == 'random_sampling':\n",
    "                # Random sampling imputation\n",
    "                data_imputed = data.copy()\n",
    "                for feature in range(data.shape[1]):\n",
    "                    missing_values_idx = np.where(np.isnan(data[:, feature]))[0]\n",
    "                    observed_values = data[~np.isnan(data[:, feature]), feature]\n",
    "                    imputed_values = np.random.choice(observed_values, size=len(missing_values_idx))\n",
    "                    data_imputed[missing_values_idx, feature] = imputed_values\n",
    "                return data_imputed, missing_values_indices\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown imputation method: {method}\")\n",
    "\n",
    "            # Apply the imputer\n",
    "            imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "            return imputed_data, missing_values_indices\n",
    "\n",
    "\n",
    "        x_unlab, missing_values_indices = impute_data(x_unlab, method=mode)\n",
    "        from keras.utils import to_categorical\n",
    "\n",
    "        # x_train = x_train.to_numpy()\n",
    "        # Logistic regression\n",
    "        y_test_hat = logit(x_train, y_train, x_test)\n",
    "        results[0] = perf_metric(metric, y_test, y_test_hat) \n",
    "\n",
    "        # XGBoost\n",
    "        y_test_hat = xgb_model(x_train, y_train, x_test)    \n",
    "        results[1] = perf_metric(metric, y_test, y_test_hat)   \n",
    "\n",
    "        # MLP\n",
    "        mlp_parameters = dict()\n",
    "        mlp_parameters['hidden_dim'] = 100\n",
    "        mlp_parameters['epochs'] = 100\n",
    "        mlp_parameters['activation'] = 'relu'\n",
    "        mlp_parameters['batch_size'] = 100\n",
    "\n",
    "        # y_train = to_categorical(y_train)\n",
    "        # y_test = to_categorical(y_test)    \n",
    "\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "        x_test = np.array(x_test)\n",
    "\n",
    "        y_test_hat = mlp(x_train, y_train, x_test, mlp_parameters)\n",
    "        results[2] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "        # Report performance\n",
    "        for m_it in range(len(model_sets)):  \n",
    "\n",
    "          model_name = model_sets[m_it]  \n",
    "\n",
    "          print('Supervised Performance, Model Name: ' + model_name + \n",
    "                ', Performance: ' + str(results[m_it]))\n",
    "        \n",
    "        # Train VIME-Self\n",
    "        vime_self_parameters = dict()\n",
    "        vime_self_parameters['batch_size'] = 128\n",
    "        vime_self_parameters['epochs'] = 10\n",
    "\n",
    "        vime_self_encoder, embeddings, all_activations, encoder_output_dim, history = get_encoder(x_unlab,architecture='default', p_m=p_m, alpha=alpha, parameters=vime_self_parameters)\n",
    "        print(\"Encoder output shape: (?, {})\".format(encoder_output_dim))\n",
    "        # encoder_output_dim = 392\n",
    "        # vime_self_encoder, embeddings, all_activations = vime_self_fnn(x_unlab, p_m, alpha, vime_self_parameters)\n",
    "\n",
    "        # Save encoder\n",
    "        if not os.path.exists('save_model'):\n",
    "          os.makedirs('save_model')\n",
    "\n",
    "        file_name = './save_model/encoder_model.h5'\n",
    "\n",
    "        vime_self_encoder.save(file_name)  \n",
    "\n",
    "        # Test VIME-Self\n",
    "        x_train_hat = vime_self_encoder.predict(x_train)\n",
    "        x_test_hat = vime_self_encoder.predict(x_test)\n",
    "\n",
    "        y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "\n",
    "        results[3] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "        print('VIME-Self Performance: ' + str(results[3]))\n",
    "        import tensorflow as tf\n",
    "\n",
    "\n",
    "        vime_semi_parameters = dict()\n",
    "        vime_semi_parameters['hidden_dim'] = 100\n",
    "        vime_semi_parameters['batch_size'] = 128\n",
    "        vime_semi_parameters['iterations'] = 1000\n",
    "        y_test_hat = vime_semi(x_train, y_train, x_unlab, x_test, \n",
    "                               vime_semi_parameters, p_m, K, beta, file_name,encoder_output_dim)\n",
    "\n",
    "        # Test VIME\n",
    "        results[4] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "        print('VIME Performance: '+ str(results[4]))\n",
    "        \n",
    "        for m_it in range(len(model_sets)):  \n",
    "\n",
    "          model_name = model_sets[m_it]  \n",
    "\n",
    "          print('Supervised Performance, Model Name: ' + model_name + \n",
    "                ', Performance: ' + str(results[m_it]))\n",
    "\n",
    "        print('VIME-Self Performance: ' + str(results[m_it+1]))\n",
    "\n",
    "        print('VIME Performance: '+ str(results[m_it+2]))\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "        print(\"*******************\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 23)\n",
      "(134, 2)\n",
      "(538, 23)\n",
      "(167, 23)\n",
      "(167, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_unlab.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "# import numpy as np\n",
    "\n",
    "# # Compute the correlation matrix using numpy\n",
    "# correlation_matrix = np.corrcoef(x_unlab, rowvar=False)\n",
    "\n",
    "# # Identify pairs of features with high absolute correlation\n",
    "# threshold = 0.7\n",
    "# high_corr_pairs = []\n",
    "# for i in range(correlation_matrix.shape[0]):\n",
    "#     for j in range(i+1, correlation_matrix.shape[1]):\n",
    "#         if abs(correlation_matrix[i, j]) > threshold:\n",
    "#             high_corr_pairs.append((i, j))\n",
    "\n",
    "# print(f\"Number of feature pairs with correlation above {threshold}: {len(high_corr_pairs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = x_unlab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def introduce_missingness(data, percentage=0.05, mechanism='MCAR'):\n",
    "    \"\"\"\n",
    "    Introduce missingness into a numpy array based on a specific mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: numpy array\n",
    "    - percentage: fraction of data that should be missing\n",
    "    - mechanism: 'MCAR', 'MAR', or 'MNAR'\n",
    "    \n",
    "    Returns:\n",
    "    - modified_data: numpy array with missing values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure data isn't an empty array\n",
    "    if data.size == 0:\n",
    "        return data\n",
    "    \n",
    "    # Convert percentage to a fraction of total data values\n",
    "    total_values = data.size\n",
    "    missing_values = int(total_values * percentage)\n",
    "    values_made_missing = 0\n",
    "    \n",
    "    # MCAR\n",
    "    if mechanism == 'MCAR':\n",
    "        # Randomly pick indices to set as missing\n",
    "        missing_indices = np.random.choice(total_values, missing_values, replace=False)\n",
    "        np.put(data, missing_indices, np.nan)\n",
    "        return data\n",
    "    \n",
    "    # MAR\n",
    "    if mechanism == 'MAR':\n",
    "        while values_made_missing < missing_values:\n",
    "            # Pick two features randomly\n",
    "            feature_1 = np.random.choice(data.shape[1], 1)\n",
    "            feature_2 = np.random.choice(data.shape[1], 1)\n",
    "            while feature_1 == feature_2:\n",
    "                feature_2 = np.random.choice(data.shape[1], 1)\n",
    "\n",
    "            threshold = np.mean(data[:, feature_1])\n",
    "            # Make values in feature_2 missing based on values in feature_1\n",
    "            potential_missing_indices = np.where(data[:, feature_1] > threshold)\n",
    "            remaining_missing = missing_values - values_made_missing\n",
    "            sample_size = min(len(potential_missing_indices[0]), remaining_missing)\n",
    "            if sample_size == 0:\n",
    "                continue\n",
    "            sample_missing = np.random.choice(len(potential_missing_indices[0]), sample_size, replace=False)\n",
    "            data[potential_missing_indices[0][sample_missing], feature_2] = np.nan\n",
    "            values_made_missing += sample_size\n",
    "        return data\n",
    "    \n",
    "    # MNAR\n",
    "    if mechanism == 'MNAR':\n",
    "        feature = np.random.choice(data.shape[1], 1)\n",
    "        threshold = np.mean(data[:, feature])\n",
    "        potential_missing_indices = np.where(data[:, feature] > threshold)\n",
    "        sample_size = min(len(potential_missing_indices[0]), missing_values)\n",
    "        sample_missing = np.random.choice(len(potential_missing_indices[0]), sample_size, replace=False)\n",
    "        data[potential_missing_indices[0][sample_missing], feature] = np.nan\n",
    "        return data\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "# data = np.random.rand(100, 5)\n",
    "x_unlab = introduce_missingness(x_unlab, percentage=0.4, mechanism='MNAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has missing values: True\n",
      "Count of missing values: 35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example array\n",
    "# arr = np.array([[1, 2, np.nan], [4, 5, 6], [7, np.nan, 9]])\n",
    "arr = x_unlab\n",
    "# Check for missing values (NaN)\n",
    "has_missing_values = np.isnan(arr).any()\n",
    "\n",
    "# Count of missing values\n",
    "missing_count = np.isnan(arr).sum()\n",
    "\n",
    "print(f\"Has missing values: {has_missing_values}\")\n",
    "print(f\"Count of missing values: {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "def impute_data(data, method='mean'):\n",
    "    # Find the indices of the missing values\n",
    "    missing_values_indices = np.argwhere(np.isnan(data))\n",
    "    \n",
    "    if method == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    elif method == 'median':\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    elif method == 'mode':\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "    elif method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "    elif method == 'iterative' or method == 'regression':\n",
    "        imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "    elif method == 'random_sampling':\n",
    "        # Random sampling imputation\n",
    "        data_imputed = data.copy()\n",
    "        for feature in range(data.shape[1]):\n",
    "            missing_values_idx = np.where(np.isnan(data[:, feature]))[0]\n",
    "            observed_values = data[~np.isnan(data[:, feature]), feature]\n",
    "            imputed_values = np.random.choice(observed_values, size=len(missing_values_idx))\n",
    "            data_imputed[missing_values_idx, feature] = imputed_values\n",
    "        return data_imputed, missing_values_indices\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown imputation method: {method}\")\n",
    "\n",
    "    # Apply the imputer\n",
    "    imputed_data = imputer.fit_transform(data)\n",
    "    \n",
    "    return imputed_data, missing_values_indices\n",
    "\n",
    "\n",
    "x_unlab, missing_values_indices = impute_data(x_unlab, method=\"random_sampling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has missing values: False\n",
      "Count of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example array\n",
    "# arr = np.array([[1, 2, np.nan], [4, 5, 6], [7, np.nan, 9]])\n",
    "arr = x_unlab\n",
    "# Check for missing values (NaN)\n",
    "has_missing_values = np.isnan(arr).any()\n",
    "\n",
    "# Count of missing values\n",
    "missing_count = np.isnan(arr).sum()\n",
    "\n",
    "print(f\"Has missing values: {has_missing_values}\")\n",
    "print(f\"Count of missing values: {missing_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Randomly shuffle the indices\n",
    "# shuffled_indices = np.random.permutation(x_unlab.shape[1])\n",
    "\n",
    "# # Select the top 50 features\n",
    "# selected_features = shuffled_indices[:23]\n",
    "\n",
    "# # Compute the correlation matrix for the subset\n",
    "# subset_corr_matrix = np.corrcoef(x_unlab[:, selected_features], rowvar=False)\n",
    "\n",
    "# # Plot the heatmap without annotations inside\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# sns.heatmap(subset_corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "# plt.title(\"Heatmap for a Subset of Features\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# # Randomly shuffle the indices\n",
    "# shuffled_indices = np.random.permutation(x_unlab.shape[1])\n",
    "\n",
    "# # Select a subset of features, for instance the top 50\n",
    "# selected_features = shuffled_indices[:23]\n",
    "\n",
    "# # Remove columns with zero variance\n",
    "# non_zero_var_columns = np.var(x_unlab[:, selected_features], axis=0) != 0\n",
    "# x_subset = x_unlab[:, selected_features][:, non_zero_var_columns]\n",
    "\n",
    "# # Compute the correlation matrix for the subset\n",
    "# subset_corr_matrix = np.corrcoef(x_subset, rowvar=False)\n",
    "\n",
    "# # Convert the correlation matrix to a distance for linkage computation\n",
    "# distances = 1 - np.abs(subset_corr_matrix)\n",
    "\n",
    "# # Compute hierarchical clustering linkage\n",
    "# link = linkage(distances, method=\"average\", optimal_ordering=True)\n",
    "\n",
    "# # Create a clustered heatmap using seaborn\n",
    "# sns.clustermap(subset_corr_matrix, row_linkage=link, col_linkage=link, cmap=\"coolwarm\", vmin=-1, vmax=1, figsize=(12, 10), annot=False)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # import seaborn as sns\n",
    "# # corr_matrix = np.nan_to_num(corr_matrix)\n",
    "\n",
    "# # sns.clustermap(corr_matrix, cmap=\"coolwarm\", figsize=(20, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install networkx matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming high_corr_pairs is already defined\n",
    "\n",
    "# # Create an empty graph\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Add edges based on high-correlation pairs\n",
    "# G.add_edges_from(high_corr_pairs)\n",
    "\n",
    "# # Draw the graph with spacing adjustments\n",
    "# plt.figure(figsize=(25, 25))\n",
    "# pos = nx.spring_layout(G, k=0.5, iterations=50)  # Increase k and iterations for more spacing and better layout\n",
    "# nx.draw_networkx_nodes(G, pos, node_size=500, alpha=0.8)\n",
    "# nx.draw_networkx_edges(G, pos, width=2.0, edge_color=\"gray\")  # Increase width and set edge color for better visibility\n",
    "# nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "# plt.title(\"Network Graph of High-correlation Feature Pairs\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Compute the correlation matrix using numpy\n",
    "# correlation_matrix = np.corrcoef(x_unlab, rowvar=False)\n",
    "\n",
    "# # Identify pairs of features with high absolute correlation\n",
    "# threshold = 0.85\n",
    "# high_corr_pairs = []\n",
    "# for i in range(correlation_matrix.shape[0]):\n",
    "#     for j in range(i+1, correlation_matrix.shape[1]):\n",
    "#         if abs(correlation_matrix[i, j]) > threshold:\n",
    "#             high_corr_pairs.append((i, j, correlation_matrix[i, j]))\n",
    "\n",
    "# print(f\"Number of feature pairs with correlation above {threshold}: {len(high_corr_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def process_datasets(x_train, x_test, x_unlab, threshold=0.9):\n",
    "#     \"\"\"\n",
    "#     Removes highly correlated features from x_train, x_test, and x_unlab.\n",
    "    \n",
    "#     Parameters:\n",
    "#         - x_train: Training data.\n",
    "#         - x_test: Testing data.\n",
    "#         - x_unlab: Unlabeled data.\n",
    "#         - threshold: Correlation threshold for feature removal.\n",
    "        \n",
    "#     Returns:\n",
    "#         - x_train_modified: Processed training data.\n",
    "#         - x_test_modified: Processed testing data.\n",
    "#         - x_unlab_modified: Processed unlabeled data.\n",
    "#         - correlated_pairs: List of tuples containing highly correlated feature pairs.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Compute the correlation matrix using the unlabeled data\n",
    "#     correlation_matrix = np.corrcoef(x_unlab, rowvar=False)\n",
    "\n",
    "#     # Identify high-correlation pairs and determine features to remove\n",
    "#     features_to_remove = set()\n",
    "#     correlated_pairs = []  # List to keep track of highly correlated pairs\n",
    "#     for i in range(correlation_matrix.shape[0]):\n",
    "#         for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "#             if abs(correlation_matrix[i, j]) > threshold:\n",
    "#                 # Decide to remove the second feature of the pair\n",
    "#                 features_to_remove.add(j)\n",
    "#                 correlated_pairs.append((i, j))  # Add the pair to the list\n",
    "\n",
    "#     # Determine features to keep\n",
    "#     features_to_keep = [i for i in range(x_unlab.shape[1]) if i not in features_to_remove]\n",
    "\n",
    "#     # Modify datasets\n",
    "#     x_train_modified = x_train[:, features_to_keep]\n",
    "#     x_test_modified = x_test[:, features_to_keep]\n",
    "#     x_unlab_modified = x_unlab[:, features_to_keep]\n",
    "\n",
    "#     return x_train_modified, x_test_modified, x_unlab_modified, correlated_pairs\n",
    "\n",
    "# # Example usage:\n",
    "# x_train_modified, x_test_modified, x_unlab_modified, correlated_pairs = process_datasets(x_train, x_test, x_unlab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_unlab=x_unlab_modified\n",
    "# x_train = x_train_modified\n",
    "# x_test= x_test_modified\n",
    "# high_corr_pairs = correlated_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_unlab.shape)\n",
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_unlabSaved = x_unlab\n",
    "# x_unlab=x_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Check if high_corr_pairs is properly generated and not empty\n",
    "# if not high_corr_pairs or len(high_corr_pairs[0]) != 3:\n",
    "#     print(\"high_corr_pairs is either empty or not generated correctly!\")\n",
    "#     exit()\n",
    "\n",
    "# # Create an empty graph\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Add edges and weights\n",
    "# edge_colors = []  # This will store the correlation values for color coding\n",
    "\n",
    "# for i, j, corr_val in high_corr_pairs:\n",
    "#     G.add_edge(i, j, weight=corr_val)\n",
    "#     edge_colors.append(corr_val)\n",
    "\n",
    "# # Normalize colors\n",
    "# min_corr = min(edge_colors)\n",
    "# max_corr = max(edge_colors)\n",
    "# edge_colors_normalized = [(c - min_corr) / (max_corr - min_corr) for c in edge_colors]\n",
    "\n",
    "# # Draw the graph with spacing adjustments\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# pos = nx.spring_layout(G, k=0.75, iterations=100)  # Adjusting k and iterations for potentially denser graph\n",
    "# # pos = nx.spring_layout(G, k=0.1, iterations=100)  # Reduce the value of k from 0.75 to 0.5 or any desired value\n",
    "\n",
    "# nx.draw_networkx_nodes(G, pos, node_size=500, alpha=0.8)\n",
    "# nx.draw_networkx_edges(G, pos, edge_color=edge_colors_normalized, edge_cmap=plt.cm.Blues, width=2.0)\n",
    "# nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "\n",
    "# # Add colorbar for edges\n",
    "# sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min_corr, vmax=max_corr))\n",
    "# plt.colorbar(sm)\n",
    "\n",
    "# plt.title(\"Network Graph of High-correlation Feature Pairs\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train supervised models\n",
    "\n",
    "- Train 3 supervised learning models (Logistic regression, XGBoost, MLP)\n",
    "- Save the performances of each supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_unlabSaved = x_unlab\n",
    "# x_unlab=x_modified\n",
    "# x_unlab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Performance, Model Name: logit, Performance: 0.7058823529411765\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7352941176470589\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.7352941176470589\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# x_train = x_train.to_numpy()\n",
    "# Logistic regression\n",
    "y_test_hat = logit(x_train, y_train, x_test)\n",
    "results[0] = perf_metric(metric, y_test, y_test_hat) \n",
    "\n",
    "# XGBoost\n",
    "y_test_hat = xgb_model(x_train, y_train, x_test)    \n",
    "results[1] = perf_metric(metric, y_test, y_test_hat)   \n",
    "\n",
    "# MLP\n",
    "mlp_parameters = dict()\n",
    "mlp_parameters['hidden_dim'] = 100\n",
    "mlp_parameters['epochs'] = 100\n",
    "mlp_parameters['activation'] = 'relu'\n",
    "mlp_parameters['batch_size'] = 100\n",
    "      \n",
    "# y_train = to_categorical(y_train)\n",
    "# y_test = to_categorical(y_test)    \n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_test_hat = mlp(x_train, y_train, x_test, mlp_parameters)\n",
    "results[2] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "# Report performance\n",
    "for m_it in range(len(model_sets)):  \n",
    "    \n",
    "  model_name = model_sets[m_it]  \n",
    "    \n",
    "  print('Supervised Performance, Model Name: ' + model_name + \n",
    "        ', Performance: ' + str(results[m_it]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "# x_unlab_visual = tsne.fit_transform(x_unlab)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D scatter plot\n",
    "# plt.scatter(x_unlab_visual[:, 0], x_unlab_visual[:, 1], s=5, alpha=0.5, c='red')\n",
    "# plt.xlabel('Component 1')\n",
    "# plt.ylabel('Component 2')\n",
    "# plt.title('2D Visualization of x_unlab_visual')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualized_cols = x_unlab[:, 100:150]\n",
    "# print(visualized_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test VIME-Self\n",
    "Train self-supervised part of VIME framework only\n",
    "- Check the performance of self-supervised part of VIME framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(x_unlab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(vime_self_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 84 samples\n",
      "Epoch 1/10\n",
      "84/84 [==============================] - 0s 1ms/sample - loss: 0.9335 - mask_loss: 0.7020 - feature_loss: 0.1158\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 0s 107us/sample - loss: 0.8022 - mask_loss: 0.6508 - feature_loss: 0.0757\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 0s 131us/sample - loss: 0.7022 - mask_loss: 0.6181 - feature_loss: 0.0421\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 0s 131us/sample - loss: 0.6647 - mask_loss: 0.6022 - feature_loss: 0.0312\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 0s 107us/sample - loss: 0.6538 - mask_loss: 0.5995 - feature_loss: 0.0271\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 0s 119us/sample - loss: 0.6499 - mask_loss: 0.5973 - feature_loss: 0.0263\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 0s 107us/sample - loss: 0.6474 - mask_loss: 0.5967 - feature_loss: 0.0253\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 0s 83us/sample - loss: 0.6460 - mask_loss: 0.5954 - feature_loss: 0.0253\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 0s 107us/sample - loss: 0.6433 - mask_loss: 0.5939 - feature_loss: 0.0247\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 0s 119us/sample - loss: 0.6419 - mask_loss: 0.5926 - feature_loss: 0.0246\n",
      "Encoder output shape: (?, 450)\n",
      "VIME-Self Performance: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "# Train VIME-Self\n",
    "vime_self_parameters = dict()\n",
    "vime_self_parameters['batch_size'] = 128\n",
    "vime_self_parameters['epochs'] = 10\n",
    "\n",
    "vime_self_encoder, embeddings, all_activations, encoder_output_dim, history = get_encoder(x_unlab,architecture='default', p_m=p_m, alpha=alpha, parameters=vime_self_parameters)\n",
    "print(\"Encoder output shape: (?, {})\".format(encoder_output_dim))\n",
    "# encoder_output_dim = 392\n",
    "# vime_self_encoder, embeddings, all_activations = vime_self_fnn(x_unlab, p_m, alpha, vime_self_parameters)\n",
    "  \n",
    "# Save encoder\n",
    "if not os.path.exists('save_model'):\n",
    "  os.makedirs('save_model')\n",
    "\n",
    "file_name = './save_model/encoder_model.h5'\n",
    "  \n",
    "vime_self_encoder.save(file_name)  \n",
    "        \n",
    "# Test VIME-Self\n",
    "x_train_hat = vime_self_encoder.predict(x_train)\n",
    "x_test_hat = vime_self_encoder.predict(x_test)\n",
    "      \n",
    "y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "\n",
    "results[3] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "print('VIME-Self Performance: ' + str(results[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def min_max_normalize(lst):\n",
    "#     \"\"\"Normalize list values to range between 0 and 1.\"\"\"\n",
    "#     return [(i-min(lst))/(max(lst)-min(lst)) for i in lst]\n",
    "\n",
    "# # Extract training and validation loss from the history object\n",
    "# train_loss = history.history['loss']\n",
    "# normalized_train_loss = min_max_normalize(train_loss)\n",
    "\n",
    "# mask_loss = history.history.get('mask_loss', None)  # Get mask loss if it exists\n",
    "# if mask_loss:\n",
    "#     normalized_mask_loss = min_max_normalize(mask_loss)\n",
    "\n",
    "# feature_loss = history.history.get('feature_loss', None)  # Get feature loss if it exists\n",
    "# if feature_loss:\n",
    "#     normalized_feature_loss = min_max_normalize(feature_loss)\n",
    "\n",
    "# # Plotting the losses\n",
    "# plt.figure(figsize=(10,6))\n",
    "\n",
    "# # Original Losses\n",
    "# plt.plot(train_loss, label='Training Loss', color='blue', linestyle='--')\n",
    "# if mask_loss:\n",
    "#     plt.plot(mask_loss, label='Mask Loss', color='red', linestyle='--')\n",
    "# if feature_loss:\n",
    "#     plt.plot(feature_loss, label='Feature Loss', color='green', linestyle='--')\n",
    "\n",
    "# # Normalized Losses\n",
    "# plt.plot(normalized_train_loss, label='Normalized Training Loss', color='blue')\n",
    "# if mask_loss:\n",
    "#     plt.plot(normalized_mask_loss, label='Normalized Mask Loss', color='red')\n",
    "# if feature_loss:\n",
    "#     plt.plot(normalized_feature_loss, label='Normalized Feature Loss', color='green')\n",
    "\n",
    "# plt.title('Losses and Normalized Losses per Epoch')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract training and validation loss from the history object\n",
    "# train_loss = history.history['loss']\n",
    "# mask_loss = history.history.get('mask_loss', None)  # Get validation loss if it exists\n",
    "# feature_loss = history.history.get('feature_loss', None)  # Get validation loss if it exists\n",
    "\n",
    "\n",
    "# # Plotting the training loss\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "# if mask_loss:  # Plot validation loss if it exists\n",
    "#     plt.plot(mask_loss, label='Mask Loss', color='red')\n",
    "\n",
    "# if feature_loss:  # Plot validation loss if it exists\n",
    "#     plt.plot(feature_loss, label='Feature Loss', color='green')\n",
    "    \n",
    "    \n",
    "# plt.title('Losses per Epoch')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vime_self_encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(10,6))\n",
    "# for encoder, loss in losses.items():\n",
    "#     plt.plot(loss, label=encoder)\n",
    "\n",
    "# plt.title(\"Loss Curves for Each Encoder\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### RUN THIS IF YOU WANT THE ENCODER CHOICE TO BE AUTOMATIC\n",
    "\n",
    "# def evaluate_architecture(architecture_name, x_unlab, p_m, alpha, vime_self_parameters, x_train, y_train, x_test, metric):\n",
    "#     # Get encoder\n",
    "#     vime_self_encoder, embeddings, all_activations, encoder_output_dim, history = get_encoder(x_unlab, architecture=architecture_name, p_m=p_m, alpha=alpha, parameters=vime_self_parameters)\n",
    "#     print(\"Encoder output shape for {}: (?, {})\".format(architecture_name, encoder_output_dim))\n",
    "\n",
    "#     # Save encoder\n",
    "#     if not os.path.exists('save_model'):\n",
    "#         os.makedirs('save_model')\n",
    "\n",
    "#     file_name = './save_model/encoder_model_{}.h5'.format(architecture_name)\n",
    "#     vime_self_encoder.save(file_name)\n",
    "\n",
    "#     # Test VIME-Self\n",
    "#     x_train_hat = vime_self_encoder.predict(x_train)\n",
    "#     x_test_hat = vime_self_encoder.predict(x_test)\n",
    "    \n",
    "#     y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "#     result = perf_metric(metric, y_test, y_test_hat)\n",
    "#     print(\"{}'S PERFORMANCE IS {}\".format(architecture_name.upper(), str(result)))\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Train VIME-Self\n",
    "# vime_self_parameters = {\n",
    "#     'batch_size': 128,\n",
    "#     'epochs': 10\n",
    "# }\n",
    "\n",
    "# architectures = ['default', 'fnn', 'autoencoder']\n",
    "# results = {}\n",
    "\n",
    "# # Evaluate each architecture\n",
    "# for arch in architectures:\n",
    "#     results[arch] = evaluate_architecture(arch, x_unlab, p_m, alpha, vime_self_parameters, x_train, y_train, x_test, metric)\n",
    "\n",
    "# # Determine the best architecture\n",
    "# best_architecture = max(results, key=results.get)\n",
    "\n",
    "\n",
    "# # Use the best architecture\n",
    "# vime_self_encoder, embeddings, all_activations, encoder_output_dim, history= get_encoder(x_unlab, architecture=best_architecture, p_m=p_m, alpha=alpha, parameters=vime_self_parameters)\n",
    "\n",
    "# # Save the final encoder\n",
    "# if not os.path.exists('save_model'):\n",
    "#     os.makedirs('save_model')\n",
    "# file_name = './save_model/encoder_model_final.h5'\n",
    "# vime_self_encoder.save(file_name)\n",
    "\n",
    "# # Final test with the best architecture\n",
    "# x_train_hat = vime_self_encoder.predict(x_train)\n",
    "# x_test_hat = vime_self_encoder.predict(x_test)\n",
    "# y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "# results[3] = perf_metric(metric, y_test, y_test_hat)\n",
    "# print('VIME-Self Performance with best architecture:', results[3])\n",
    "# print(\"THE CHOSEN ARCHITECTURE IS\", best_architecture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0 \n",
    "\n",
    "# for layer_name, activation in all_activations.items():\n",
    "#     print(layer_name)\n",
    "#     counter = counter+1  \n",
    "    \n",
    "#     print(activation) \n",
    "    \n",
    "# print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chosen_activations[:, neuron1_index].shape)\n",
    "# print(chosen_activations[:, neuron2_index].shape)\n",
    "# print(x_unlab[:, chosen_feature_index].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extracting activations using your function\n",
    "# activations_dict = all_activations\n",
    "\n",
    "# # Choose a layer for visualization, for example, the first dense layer\n",
    "# # (You can adjust this according to the layer names you have in your model)\n",
    "# chosen_layer_name = 'dense_3'  # example name, replace with actual layer name\n",
    "# chosen_activations = activations_dict[chosen_layer_name]\n",
    "\n",
    "# # Define indices for neurons you'd like to visualize\n",
    "# neuron1_index = 0\n",
    "# neuron2_index = 1\n",
    "\n",
    "# # Chosen feature index from original data for coloring\n",
    "# chosen_feature_index = 0\n",
    "\n",
    "# # Visualization\n",
    "# plt.scatter(chosen_activations[:, neuron1_index], chosen_activations[:, neuron2_index], c=x_unlab[:, chosen_feature_index], cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.title(f'Activations of {chosen_layer_name} and Feature Visualization')\n",
    "# plt.xlabel(f'Activation Neuron {neuron1_index}')\n",
    "# plt.ylabel(f'Activation Neuron {neuron2_index}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations_dict[chosen_layer_name].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron_indices = get_random_subset_indices(activations_dict[chosen_layer_name].shape[1], neuron_fraction)\n",
    "# neuron_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# activations_dict = all_activations\n",
    "# chosen_layer_name = 'dense_3'\n",
    "\n",
    "\n",
    "\n",
    "# def compute_correlation_with_features(activations, features):\n",
    "#     \"\"\"Compute correlation between activations and features.\"\"\"\n",
    "#     num_neurons = activations.shape[1]\n",
    "#     num_features = features.shape[1]\n",
    "    \n",
    "#     correlation_matrix = np.zeros((num_neurons, num_features))\n",
    "    \n",
    "#     for i in range(num_neurons):\n",
    "#         for j in range(num_features):\n",
    "#             correlation_matrix[i, j] = np.corrcoef(activations[:, i], features[:, j])[0, 1]\n",
    "            \n",
    "#     return correlation_matrix\n",
    "\n",
    "# def get_random_subset_indices(total_length, fraction):\n",
    "#     \"\"\"Get a random subset of indices based on the given fraction.\"\"\"\n",
    "#     subset_length = int(total_length * fraction)\n",
    "#     return np.random.choice(total_length, subset_length, replace=False)\n",
    "\n",
    "# # Fraction of neurons and features you wish to visualize\n",
    "# neuron_fraction = 1\n",
    "# feature_fraction = 1\n",
    "\n",
    "# # neuron_indices = get_random_subset_indices(embeddings.shape[1], neuron_fraction)\n",
    "# neuron_indices = get_random_subset_indices(activations_dict[chosen_layer_name].shape[1], neuron_fraction)\n",
    "# feature_indices = get_random_subset_indices(x_unlab.shape[1], feature_fraction)\n",
    "\n",
    "# chosen_activations = activations_dict[chosen_layer_name][:, neuron_indices]\n",
    "# subset_features = x_unlab[:, feature_indices]\n",
    "# correlation_matrix = compute_correlation_with_features(chosen_activations, subset_features)\n",
    "\n",
    "# # Visualization\n",
    "# plt.figure(figsize=(25, 18))\n",
    "# sns.heatmap(correlation_matrix, cmap='viridis', annot=True)\n",
    "# plt.title(f'Correlation between Random Subset of Activations of {chosen_layer_name} and Features')\n",
    "# plt.xlabel('All Features')\n",
    "# plt.ylabel('All Neurons')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Assuming `activations` is a matrix where rows are samples and columns are activations\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(activation, cmap='viridis')\n",
    "# plt.title('Activation Heatmap')\n",
    "# plt.xlabel('Neurons')\n",
    "# plt.ylabel('Samples')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming activations is a matrix and original_data is your feature data\n",
    "# plt.scatter(activation[:, neuron1], activations[:, neuron2], c=original_data[:, chosen_feature], cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.title('Activations and Feature Visualization')\n",
    "# plt.xlabel(f'Activation Neuron {neuron1}')\n",
    "# plt.ylabel(f'Activation Neuron {neuron2}')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = np.mean(embeddings, axis=0)\n",
    "# variances = np.var(embeddings, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# plt.hist(means, bins=30, edgecolor='black', alpha=0.7) # you can adjust the number of bins as per your requirements\n",
    "# plt.title(\"Distribution of Neuron Activation Means\")\n",
    "# plt.xlabel(\"Mean Activation Value\")\n",
    "# plt.ylabel(\"Number of Neurons\")\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# # plt.xlim([0.0, 1.5])  # adjust the x-axis range here\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Replace this placeholder with your actual variance data\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.hist(variances, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "# plt.title(\"Distribution of Neuron Activation Variances\")\n",
    "# plt.xlabel(\"Variance Value\")\n",
    "# plt.ylabel(\"Number of Neurons\")\n",
    "# plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = embeddings \n",
    "\n",
    "# row_drop_fraction = 0.95  # For example, drop 20% of rows\n",
    "# col_drop_fraction = 0.95  # For example, drop 30% of columns\n",
    "\n",
    "# # Calculate the number of rows and columns to drop\n",
    "# num_rows_to_drop = int(row_drop_fraction * data.shape[0])\n",
    "# num_cols_to_drop = int(col_drop_fraction * data.shape[1])\n",
    "\n",
    "# # Shuffle the row indices and column indices\n",
    "# row_indices = np.arange(data.shape[0])\n",
    "# col_indices = np.arange(data.shape[1])\n",
    "# np.random.shuffle(row_indices)\n",
    "# np.random.shuffle(col_indices)\n",
    "\n",
    "# # Select the remaining rows and columns\n",
    "# remaining_rows = row_indices[num_rows_to_drop:]\n",
    "# remaining_cols = col_indices[num_cols_to_drop:]\n",
    "\n",
    "# # Drop the specified rows and columns\n",
    "# data_after_drop = data[remaining_rows][:, remaining_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KernelDensity\n",
    "# import numpy as np\n",
    "\n",
    "# # Sample data\n",
    "# activations = np.random.randn(5400, 79)\n",
    "\n",
    "# # Compute KDE for each feature\n",
    "# kde_list = []\n",
    "# for i in range(activations.shape[1]):\n",
    "#     kde = KernelDensity(kernel='gaussian', bandwidth=0.5)\n",
    "#     kde.fit(activations[:, i][:, np.newaxis])\n",
    "#     kde_list.append(kde)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x = np.linspace(-5, 5, 1000)  # Adjust the range as per your data\n",
    "\n",
    "# plt.figure(figsize=(7, 5))\n",
    "\n",
    "# # Subset of features for clarity\n",
    "# features_to_plot = range(10)  # Plotting only the first 10 features as an example\n",
    "\n",
    "# for i in features_to_plot:\n",
    "#     y = np.exp(kde_list[i].score_samples(x[:, np.newaxis]))\n",
    "#     plt.plot(x, y, label=f\"Feature {i}\")\n",
    "\n",
    "# plt.xlim([-0.02, 0.02])  # Adjust as needed\n",
    "# plt.ylim([0, 0.5])  # Adjust as needed\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title(\"Density plots for activations\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_after_drop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# def plot_kde(activations, num_features_to_plot=40):\n",
    "#     # Plot KDE for the specified number of features\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     for i in range(num_features_to_plot):\n",
    "#         sns.kdeplot(activations[:, i], label=f'Neuron {i+1}')\n",
    "    \n",
    "#     plt.xlabel('Activation Value')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.title('KDE of Neuron Activations')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "# # Call the function with your activations\n",
    "# plot_kde(data_after_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embeddings=embeddings.numpy()\n",
    "\n",
    "# print(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Assuming 'embeddings' is your numpy array with high-dimensional data\n",
    "# pca = PCA(n_components=2)\n",
    "# reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D scatter plot\n",
    "# plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=5, alpha=0.06) \n",
    "# plt.xlabel('Component 1')\n",
    "# plt.ylabel('Component 2')\n",
    "# plt.title('2D Visualization of Embeddings')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "# reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D scatter plot\n",
    "# plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=15, alpha=1)\n",
    "# plt.xlabel('Component 1')\n",
    "# plt.ylabel('Component 2')\n",
    "# plt.title('2D Visualization of Embeddings')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.kdeplot(x_unlab_visual[:, 0], x_unlab_visual[:, 1], alpha=0.8, cmap='Reds', shade=True, label='Scatter Plot 1')\n",
    "# sns.kdeplot(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.4, cmap='Blues', shade=True, label='Scatter Plot 2')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title('Comparison of Density Plots')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.kdeplot(reduced_embeddings[:, 0], reduced_embeddings[:, 1], alpha=0.8, cmap='Blues', shade=True, label='Scatter Plot 2')\n",
    "# plt.legend()\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title('Comparison of Density Plots')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import Isomap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Instantiate and fit the Isomap model\n",
    "# isomap = Isomap(n_components=2, n_neighbors=5)\n",
    "# isomap_results = isomap.fit_transform(embeddings)\n",
    "\n",
    "# # Plotting\n",
    "# plt.scatter(isomap_results[:, 0], isomap_results[:, 1], s=5, alpha=0.06)\n",
    "# plt.xlabel('Dimension 1')\n",
    "# plt.ylabel('Dimension 2')\n",
    "# plt.title('Isomap Visualization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('C:\\\\Users\\\\georg\\\\AppData\\\\Roaming\\\\Python\\\\Lib\\\\site-packages')\n",
    "# import umap\n",
    "# # C:\\Users\\georg\\AppData\\Roaming\\Python\n",
    "\n",
    "# import umap\n",
    "\n",
    "# reducer = umap.UMAP()\n",
    "# reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 2D scatter plot\n",
    "# plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=5, alpha=0.06)\n",
    "# plt.xlabel('Component 1')\n",
    "# plt.ylabel('Component 2')\n",
    "# plt.title('2D Visualization of Embeddings')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test VIME\n",
    "\n",
    "Train semi-supervised part of VIME framework on top of trained self-supervised encoder\n",
    "- Check the performance of entire part of VIME framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\georg\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:94: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\georg\\anaconda33\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:94: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Iteration: 0/1000, Current loss: 0.8873\n",
      "Iteration: 100/1000, Current loss: 0.4405\n",
      "Iteration: 200/1000, Current loss: 0.525\n",
      "Iteration: 300/1000, Current loss: 0.5233\n",
      "INFO:tensorflow:Restoring parameters from ./save_model/class_model.ckpt\n",
      "VIME Performance: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "# Train VIME-Semi\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "vime_semi_parameters = dict()\n",
    "vime_semi_parameters['hidden_dim'] = 100\n",
    "vime_semi_parameters['batch_size'] = 128\n",
    "vime_semi_parameters['iterations'] = 1000\n",
    "y_test_hat = vime_semi(x_train, y_train, x_unlab, x_test, \n",
    "                       vime_semi_parameters, p_m, K, beta, file_name,encoder_output_dim)\n",
    "\n",
    "# Test VIME\n",
    "results[4] = perf_metric(metric, y_test, y_test_hat)\n",
    "  \n",
    "print('VIME Performance: '+ str(results[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Prediction Performances\n",
    "\n",
    "- 3 Supervised learning models\n",
    "- VIME with self-supervised part only\n",
    "- Entire VIME framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised Performance, Model Name: logit, Performance: 0.7058823529411765\n",
      "Supervised Performance, Model Name: xgboost, Performance: 0.7352941176470589\n",
      "Supervised Performance, Model Name: mlp, Performance: 0.7352941176470589\n",
      "VIME-Self Performance: 0.6764705882352942\n",
      "VIME Performance: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "for m_it in range(len(model_sets)):  \n",
    "    \n",
    "  model_name = model_sets[m_it]  \n",
    "    \n",
    "  print('Supervised Performance, Model Name: ' + model_name + \n",
    "        ', Performance: ' + str(results[m_it]))\n",
    "    \n",
    "print('VIME-Self Performance: ' + str(results[m_it+1]))\n",
    "  \n",
    "print('VIME Performance: '+ str(results[m_it+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
